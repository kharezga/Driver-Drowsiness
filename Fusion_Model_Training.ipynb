{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "492a3578",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ccd3d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Platform: macOS-13.3.1-arm64-arm-64bit\n",
      "Tensor Flow Version: 2.9.0\n",
      "Keras Version: 2.9.0\n",
      "\n",
      "Python 3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:25:29) [Clang 14.0.6 ]\n",
      "Scikit-Learn 1.2.0\n",
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "import sys\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Flatten, MaxPooling2D, TimeDistributed\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score\n",
    "import sklearn as sk\n",
    "from keras import regularizers\n",
    "import platform\n",
    "\n",
    "\n",
    "print(f\"Python Platform: {platform.platform()}\")\n",
    "print(f\"Tensor Flow Version: {tf.__version__}\")\n",
    "print(f\"Keras Version: {tensorflow.keras.__version__}\")\n",
    "print()\n",
    "print(f\"Python {sys.version}\")\n",
    "print(f\"Scikit-Learn {sk.__version__}\")\n",
    "gpu = len(tf.config.list_physical_devices('GPU'))>0\n",
    "print(\"GPU is\", \"available\" if gpu else \"NOT AVAILABLE\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a62eac5e",
   "metadata": {},
   "source": [
    "##  Load frames"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e5ee6305",
   "metadata": {},
   "source": [
    "#### Load new dataset standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "312c1237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File number: 1 processed for level Yawning\n",
      "File number: 2 processed for level Yawning\n",
      "File number: 3 processed for level Yawning\n",
      "File number: 4 processed for level Yawning\n",
      "File number: 5 processed for level Yawning\n",
      "File number: 6 processed for level Yawning\n",
      "File number: 7 processed for level Yawning\n",
      "File number: 8 processed for level Yawning\n",
      "File number: 9 processed for level Yawning\n",
      "File number: 10 processed for level Yawning\n",
      "File number: 11 processed for level Yawning\n",
      "File number: 12 processed for level Yawning\n",
      "File number: 13 processed for level Yawning\n",
      "File number: 14 processed for level Yawning\n",
      "File number: 15 processed for level Yawning\n",
      "File number: 16 processed for level Yawning\n",
      "File number: 17 processed for level Yawning\n",
      "File number: 18 processed for level Yawning\n",
      "File number: 19 processed for level Yawning\n",
      "File number: 20 processed for level Yawning\n",
      "File number: 21 processed for level Yawning\n",
      "File number: 22 processed for level Yawning\n",
      "File number: 23 processed for level Yawning\n",
      "File number: 24 processed for level Yawning\n",
      "File number: 25 processed for level Yawning\n",
      "File number: 26 processed for level Yawning\n",
      "File number: 27 processed for level Yawning\n",
      "File number: 28 processed for level Yawning\n",
      "File number: 29 processed for level Yawning\n",
      "File number: 30 processed for level Yawning\n",
      "File number: 31 processed for level Yawning\n",
      "File number: 32 processed for level Yawning\n",
      "File number: 33 processed for level Yawning\n",
      "File number: 34 processed for level Yawning\n",
      "File number: 35 processed for level Yawning\n",
      "File number: 36 processed for level Yawning\n",
      "File number: 37 processed for level Yawning\n",
      "File number: 38 processed for level Yawning\n",
      "File number: 39 processed for level Yawning\n",
      "File number: 40 processed for level Yawning\n",
      "File number: 41 processed for level Yawning\n",
      "File number: 42 processed for level Yawning\n",
      "File number: 43 processed for level Yawning\n",
      "File number: 44 processed for level Yawning\n",
      "File number: 45 processed for level Yawning\n",
      "File number: 46 processed for level Yawning\n",
      "File number: 47 processed for level Yawning\n",
      "File number: 48 processed for level Yawning\n",
      "File number: 49 processed for level Yawning\n",
      "File number: 50 processed for level Yawning\n",
      "File number: 51 processed for level Yawning\n",
      "File number: 52 processed for level Yawning\n",
      "File number: 53 processed for level Yawning\n",
      "File number: 54 processed for level Yawning\n",
      "File number: 55 processed for level Yawning\n",
      "File number: 56 processed for level Yawning\n",
      "File number: 57 processed for level Yawning\n",
      "File number: 58 processed for level Yawning\n",
      "File number: 59 processed for level Yawning\n",
      "File number: 60 processed for level Yawning\n",
      "File number: 61 processed for level Yawning\n",
      "File number: 62 processed for level Yawning\n",
      "File number: 63 processed for level Yawning\n",
      "File number: 64 processed for level Yawning\n",
      "File number: 65 processed for level Yawning\n",
      "File number: 66 processed for level Yawning\n",
      "File number: 67 processed for level Yawning\n",
      "File number: 68 processed for level Yawning\n",
      "File number: 69 processed for level Yawning\n",
      "File number: 70 processed for level Yawning\n",
      "File number: 71 processed for level Yawning\n",
      "File number: 72 processed for level Yawning\n",
      "File number: 73 processed for level Yawning\n",
      "File number: 74 processed for level Yawning\n",
      "File number: 75 processed for level Yawning\n",
      "File number: 76 processed for level Yawning\n",
      "File number: 77 processed for level Yawning\n",
      "File number: 78 processed for level Yawning\n",
      "File number: 79 processed for level Yawning\n",
      "File number: 80 processed for level Yawning\n",
      "File number: 81 processed for level Yawning\n",
      "File number: 82 processed for level Yawning\n",
      "File number: 83 processed for level Yawning\n",
      "File number: 84 processed for level Yawning\n",
      "File number: 85 processed for level Yawning\n",
      "File number: 86 processed for level Yawning\n",
      "File number: 87 processed for level Yawning\n",
      "File number: 88 processed for level Yawning\n",
      "File number: 89 processed for level Yawning\n",
      "File number: 1 processed for level NonYawning\n",
      "File number: 2 processed for level NonYawning\n",
      "File number: 3 processed for level NonYawning\n",
      "File number: 4 processed for level NonYawning\n",
      "File number: 5 processed for level NonYawning\n",
      "File number: 6 processed for level NonYawning\n",
      "File number: 7 processed for level NonYawning\n",
      "File number: 8 processed for level NonYawning\n",
      "File number: 9 processed for level NonYawning\n",
      "File number: 10 processed for level NonYawning\n",
      "File number: 11 processed for level NonYawning\n",
      "File number: 12 processed for level NonYawning\n",
      "File number: 13 processed for level NonYawning\n",
      "File number: 14 processed for level NonYawning\n",
      "File number: 15 processed for level NonYawning\n",
      "File number: 16 processed for level NonYawning\n",
      "File number: 17 processed for level NonYawning\n",
      "File number: 18 processed for level NonYawning\n",
      "File number: 19 processed for level NonYawning\n",
      "File number: 20 processed for level NonYawning\n",
      "File number: 21 processed for level NonYawning\n",
      "File number: 22 processed for level NonYawning\n",
      "File number: 23 processed for level NonYawning\n",
      "File number: 24 processed for level NonYawning\n",
      "File number: 25 processed for level NonYawning\n",
      "File number: 26 processed for level NonYawning\n",
      "File number: 27 processed for level NonYawning\n",
      "File number: 28 processed for level NonYawning\n",
      "File number: 29 processed for level NonYawning\n",
      "File number: 30 processed for level NonYawning\n",
      "File number: 31 processed for level NonYawning\n",
      "File number: 32 processed for level NonYawning\n",
      "File number: 33 processed for level NonYawning\n",
      "File number: 34 processed for level NonYawning\n",
      "File number: 35 processed for level NonYawning\n",
      "File number: 36 processed for level NonYawning\n",
      "File number: 37 processed for level NonYawning\n",
      "File number: 38 processed for level NonYawning\n",
      "File number: 39 processed for level NonYawning\n",
      "File number: 40 processed for level NonYawning\n",
      "File number: 41 processed for level NonYawning\n",
      "File number: 42 processed for level NonYawning\n",
      "File number: 43 processed for level NonYawning\n",
      "File number: 44 processed for level NonYawning\n",
      "File number: 45 processed for level NonYawning\n",
      "File number: 46 processed for level NonYawning\n",
      "File number: 47 processed for level NonYawning\n",
      "File number: 48 processed for level NonYawning\n",
      "File number: 49 processed for level NonYawning\n",
      "File number: 50 processed for level NonYawning\n",
      "File number: 51 processed for level NonYawning\n",
      "File number: 52 processed for level NonYawning\n",
      "File number: 53 processed for level NonYawning\n",
      "File number: 54 processed for level NonYawning\n",
      "File number: 55 processed for level NonYawning\n",
      "File number: 56 processed for level NonYawning\n",
      "File number: 57 processed for level NonYawning\n",
      "File number: 58 processed for level NonYawning\n",
      "File number: 59 processed for level NonYawning\n",
      "File number: 60 processed for level NonYawning\n",
      "File number: 61 processed for level NonYawning\n",
      "File number: 62 processed for level NonYawning\n",
      "File number: 63 processed for level NonYawning\n",
      "File number: 64 processed for level NonYawning\n",
      "File number: 65 processed for level NonYawning\n",
      "File number: 66 processed for level NonYawning\n",
      "File number: 67 processed for level NonYawning\n",
      "File number: 68 processed for level NonYawning\n",
      "File number: 69 processed for level NonYawning\n",
      "File number: 70 processed for level NonYawning\n",
      "File number: 71 processed for level NonYawning\n",
      "File number: 72 processed for level NonYawning\n",
      "File number: 73 processed for level NonYawning\n",
      "File number: 74 processed for level NonYawning\n",
      "File number: 75 processed for level NonYawning\n",
      "File number: 76 processed for level NonYawning\n",
      "File number: 77 processed for level NonYawning\n",
      "File number: 78 processed for level NonYawning\n",
      "File number: 79 processed for level NonYawning\n",
      "File number: 80 processed for level NonYawning\n",
      "File number: 81 processed for level NonYawning\n",
      "File number: 82 processed for level NonYawning\n",
      "File number: 83 processed for level NonYawning\n",
      "File number: 84 processed for level NonYawning\n",
      "File number: 85 processed for level NonYawning\n",
      "File number: 86 processed for level NonYawning\n",
      "File number: 87 processed for level NonYawning\n",
      "File number: 88 processed for level NonYawning\n",
      "File number: 89 processed for level NonYawning\n",
      "Processed! Current dataset shape: (178, 179, 1434)\n"
     ]
    }
   ],
   "source": [
    "drowsines_levels = np.array([\"Yawning\", \"NonYawning\"])\n",
    "DATA_PATH = os.path.join('./Yawning_Extracted_Values')\n",
    "label_map = {label: num for num, label in enumerate(drowsines_levels)}\n",
    "\n",
    "files, labels = [], []\n",
    "\n",
    "for level in drowsines_levels:\n",
    "    for video_index in range(1,90):\n",
    "        window = []      \n",
    "        for frame in range(1, 180):\n",
    "            file_path = os.path.join(DATA_PATH, level, str(video_index).zfill(3), (str(frame) + \".npy\"))\n",
    "            if os.path.exists(file_path):\n",
    "                res = np.load(file_path)        \n",
    "                window.append(res)\n",
    "            else:\n",
    "                print(f\"File at {file_path} doesn't exists\")\n",
    "        files.append(window)\n",
    "        labels.append(label_map[level])\n",
    "        print(f\"File number: {video_index} processed for level {level}\")\n",
    "\n",
    "print(f\"Processed! Current dataset shape: {np.array(files).shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8fcc6821",
   "metadata": {},
   "source": [
    "#### Load new dataset fancy progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c22c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processDataset():\n",
    "    drowsines_levels = np.array([\"Sleepy\", \"NonSleepy\"])\n",
    "    DATA_PATH = os.path.join('./Extracted_Values_Bigger_Confidence')\n",
    "    label_map = {label: num for num, label in enumerate(drowsines_levels)}\n",
    "\n",
    "    files, labels = [], []\n",
    "\n",
    "    for level in drowsines_levels:\n",
    "        for video_index in range(1,90):\n",
    "            window = []      \n",
    "            for frame in range(1, 1001):\n",
    "                file_path = os.path.join(DATA_PATH, level, str(video_index), (str(frame) + \".npy\"))\n",
    "                if os.path.exists(file_path):\n",
    "                    res = np.load(file_path)        \n",
    "                    window.append(res)\n",
    "                else:\n",
    "                    print(f\"File at {file_path} doesn't exists\")\n",
    "            files.append(window)\n",
    "            labels.append(label_map[level])\n",
    "            progress_percentage = (video_index / 89) * 100  # Calculate the percentage progress\n",
    "            status = f\"Progress: {progress_percentage:.2f}% | Level: {level}\"\n",
    "            print(status, end=\"\\r\")  # Overwrite the previous status\n",
    "\n",
    "    print(f\"Processed! Current dataset shape: {np.array(files).shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcfbd964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178, 1000, 1434)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(files).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e388f0e6",
   "metadata": {},
   "source": [
    "## Yawning Model training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9c9de6ff",
   "metadata": {},
   "source": [
    "## Prapare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa9df52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "objc[15345]: Class CaptureDelegate is implemented in both /Users/M374155/miniforge3/lib/python3.10/site-packages/cv2/cv2.abi3.so (0x17ed465a0) and /Users/M374155/miniforge3/lib/python3.10/site-packages/mediapipe/.dylibs/libopencv_videoio.3.4.16.dylib (0x16a5ec860). One of the two will be used. Which one is undefined.\n",
      "objc[15345]: Class CVWindow is implemented in both /Users/M374155/miniforge3/lib/python3.10/site-packages/cv2/cv2.abi3.so (0x17ed465f0) and /Users/M374155/miniforge3/lib/python3.10/site-packages/mediapipe/.dylibs/libopencv_highgui.3.4.16.dylib (0x1175eca68). One of the two will be used. Which one is undefined.\n",
      "objc[15345]: Class CVView is implemented in both /Users/M374155/miniforge3/lib/python3.10/site-packages/cv2/cv2.abi3.so (0x17ed46618) and /Users/M374155/miniforge3/lib/python3.10/site-packages/mediapipe/.dylibs/libopencv_highgui.3.4.16.dylib (0x1175eca90). One of the two will be used. Which one is undefined.\n",
      "objc[15345]: Class CVSlider is implemented in both /Users/M374155/miniforge3/lib/python3.10/site-packages/cv2/cv2.abi3.so (0x17ed46640) and /Users/M374155/miniforge3/lib/python3.10/site-packages/mediapipe/.dylibs/libopencv_highgui.3.4.16.dylib (0x1175ecab8). One of the two will be used. Which one is undefined.\n"
     ]
    }
   ],
   "source": [
    "import cv2 as cv\n",
    "import os\n",
    "from os.path import exists\n",
    "import numpy as np\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9523fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractKeypoints(result):\n",
    "    if result.multi_face_landmarks:\n",
    "        for face_detected in result.multi_face_landmarks:\n",
    "            face = np.array([[res.x, res.y, res.z] for res in face_detected.landmark]).flatten()\n",
    "    else:\n",
    "        face = np.zeros(1434)\n",
    "\n",
    "    return face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76ccf991",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawLandmarks(mp_face_mesh, results, frame): \n",
    "    mp_drawing = mp.solutions.drawing_utils  \n",
    "    mp_drawing_styles = mp.solutions.drawing_styles\n",
    "    if results.multi_face_landmarks:\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "            mp_drawing.draw_landmarks(image=frame, landmark_list=face_landmarks,\n",
    "                                       connections=mp_face_mesh.FACEMESH_TESSELATION, \n",
    "                                       landmark_drawing_spec=None,\n",
    "                                        connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_tesselation_style())\n",
    "            mp_drawing.draw_landmarks(image=frame, landmark_list=face_landmarks,\n",
    "                                       connections=mp_face_mesh.FACEMESH_CONTOURS,\n",
    "                                         landmark_drawing_spec=None,\n",
    "                                           connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_contours_style())\n",
    "            mp_drawing.draw_landmarks(image=frame, landmark_list=face_landmarks,\n",
    "                                       connections=mp_face_mesh.FACEMESH_IRISES,\n",
    "                                         landmark_drawing_spec=None,\n",
    "                                           connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_iris_connections_style())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d0a580c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def revertColors(frame, mesh): \n",
    "    image = cv.cvtColor(frame, cv.COLOR_BGR2RGB)  \n",
    "    image.flags.writeable = False \n",
    "    results = mesh.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image = cv.cvtColor(image, cv.COLOR_RGB2BGR)  \n",
    "    \n",
    "    return image, results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = os.path.join('./Yawning//')\n",
    "\n",
    "for state in [\"Yawning\", \"NonYawning\"]:\n",
    "    for number in range(1,91):\n",
    "            videoIndex  = str(number).zfill(3)\n",
    "            try:\n",
    "                os.makedirs(os.path.join(DATA_PATH, state , str(videoIndex)))\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b822eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_drawing = mp.solutions.drawing_utils  \n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "videoNumberRange = [1, 36, 2, 6, 8, 9, 12, 13, 15, 20, 23, 24, 31, 32, 33, 34, 35, 5]\n",
    "\n",
    "saveVideoIndex = 1\n",
    "for videoNumber in videoNumberRange:\n",
    "    for state in [\"glasses\", \"noglasses\", \"night_noglasses\", \"sunglasses\", \"nightglasses\"]:\n",
    "        videoIndex = str(videoNumber).zfill(3)\n",
    "        videoPath = os.path.join('/Users/M374155/Desktop/New Dataset/Training_Evaluation_Dataset/Training Dataset/' +\n",
    "                                 videoIndex + \"/\" + state + \"/yawning.avi\")\n",
    "        cap = cv.VideoCapture(videoPath)\n",
    "        textFileName = \"_yawning_mouth.txt\"\n",
    "        textFilePath = os.path.join('/Users/M374155/Desktop/New Dataset/Training_Evaluation_Dataset/Training Dataset/' +\n",
    "                                    videoIndex + \"/\" + state + \"/\" + videoIndex + textFileName)\n",
    "        print(\"Processing video number: \", str(videoNumber), \" for state \", state, \" Save Index:  \", saveVideoIndex)\n",
    "\n",
    "        with open(textFilePath, 'r') as file:\n",
    "            yawningFrameCounter = 0\n",
    "            nonYawningFrameCounter = 0\n",
    "            for char in file.read():\n",
    "                if char == \"1\":\n",
    "                    savePath = os.path.join(\"./Yawning/Yawning/\", str(saveVideoIndex).zfill(3), str(yawningFrameCounter))\n",
    "                    yawningFrameCounter += 1\n",
    "                else:\n",
    "                    savePath = os.path.join(\"./Yawning/NonYawning/\", str(saveVideoIndex).zfill(3), str(nonYawningFrameCounter))\n",
    "                    nonYawningFrameCounter += 1\n",
    "\n",
    "                with mp_face_mesh.FaceMesh(\n",
    "                                refine_landmarks=True,\n",
    "                                min_detection_confidence=0.6,\n",
    "                                min_tracking_confidence=0.7) as mesh:\n",
    "                    \n",
    "                    ret, frame = cap.read()               \n",
    "                    image, results = revertColors(frame, mesh)\n",
    "                    drawLandmarks(mp_face_mesh, results, image)\n",
    "                    landmarks = extractKeypoints(results)\n",
    "\n",
    "                    np.save(savePath, landmarks)\n",
    "                    cv.imshow('Video', image)\n",
    "                                    \n",
    "                    if cv.waitKey(1) == ord('q'):\n",
    "                        break              \n",
    "\n",
    "        cap.release()\n",
    "        saveVideoIndex += 1\n",
    "cv.destroyAllWindows()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "18b9d8b8",
   "metadata": {},
   "source": [
    "## Extract eyes values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71f49375",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "DATA_PATH = os.path.join('./Eyes_Extracted_Values//')\n",
    "\n",
    "for state in [\"Sleepy\", \"NonSleepy\"]:\n",
    "    for number in range(1,91):\n",
    "            videoIndex  = str(number).zfill(3)\n",
    "            try:\n",
    "                os.makedirs(os.path.join(DATA_PATH, state , str(videoIndex)))\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc88e6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_drawing = mp.solutions.drawing_utils  \n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "videoNumberRange = [1, 36, 2, 6, 8, 9, 12, 13, 15, 20, 23, 24, 31, 32, 33, 34, 35, 5]\n",
    "\n",
    "saveVideoIndex = 1\n",
    "for videoNumber in videoNumberRange:\n",
    "    for state in [\"glasses\", \"noglasses\", \"night_noglasses\", \"sunglasses\", \"nightglasses\"]:\n",
    "        videoIndex = str(videoNumber).zfill(3)\n",
    "        videoPath = os.path.join('/Users/M374155/Desktop/New Dataset/Training_Evaluation_Dataset/Training Dataset/' +\n",
    "                                 videoIndex + \"/\" + state + \"/slowBlinkWithNodding.avi\")\n",
    "        cap = cv.VideoCapture(videoPath)\n",
    "        textFileName = \"_slowBlinkWithNodding_eye.txt\"\n",
    "        textFilePath = os.path.join('/Users/M374155/Desktop/New Dataset/Training_Evaluation_Dataset/Training Dataset/' +\n",
    "                                    videoIndex + \"/\" + state + \"/\" + videoIndex + textFileName)\n",
    "        print(\"Processing video number: \", str(videoNumber), \" for state \", state, \" Save Index:  \", saveVideoIndex)\n",
    "\n",
    "        with open(textFilePath, 'r') as file:\n",
    "            sleepyEyesFrameCounter = 0\n",
    "            nonSleepyEyesFrameCounter = 0\n",
    "            for char in file.read():\n",
    "                if char == \"1\":\n",
    "                    savePath = os.path.join(\"./Eyes_Extracted_Values/Sleepy/\", str(saveVideoIndex).zfill(3), str(sleepyEyesFrameCounter))\n",
    "                    sleepyEyesFrameCounter += 1\n",
    "                else:\n",
    "                    savePath = os.path.join(\"./Eyes_Extracted_Values/NonSleepy/\", str(saveVideoIndex).zfill(3), str(nonSleepyEyesFrameCounter))\n",
    "                    nonSleepyEyesFrameCounter += 1\n",
    "\n",
    "                with mp_face_mesh.FaceMesh(\n",
    "                                refine_landmarks=True,\n",
    "                                min_detection_confidence=0.6,\n",
    "                                min_tracking_confidence=0.7) as mesh:\n",
    "                    \n",
    "                    ret, frame = cap.read()               \n",
    "                    image, results = revertColors(frame, mesh)\n",
    "                    drawLandmarks(mp_face_mesh, results, image)\n",
    "                    landmarks = extractKeypoints(results)\n",
    "\n",
    "                    np.save(savePath, landmarks)\n",
    "                    cv.imshow('Video', image)\n",
    "                                    \n",
    "                    if cv.waitKey(1) == ord('q'):\n",
    "                        break              \n",
    "\n",
    "        cap.release()\n",
    "        saveVideoIndex += 1\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9ca64322",
   "metadata": {},
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9be1ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_24\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_72 (LSTM)              (None, 1000, 64)          383744    \n",
      "                                                                 \n",
      " lstm_73 (LSTM)              (None, 1000, 128)         98816     \n",
      "                                                                 \n",
      " lstm_74 (LSTM)              (None, 32)                20608     \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 64)                2112      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 505,410\n",
      "Trainable params: 505,410\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True, activation='tanh', input_shape=(1000, 1434)))\n",
    "model.add(LSTM(128, return_sequences=True, activation='tanh')) \n",
    "model.add(LSTM(32, return_sequences=False, activation='tanh')) \n",
    "model.add(Dense(64, activation='softmax', kernel_regularizer='l2')) \n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(drowsines_levels.shape[0], activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "22460caf",
   "metadata": {},
   "source": [
    "#### Train Test Split Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "date_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "\n",
    "X = np.array(files)\n",
    "y = to_categorical(labels).astype(int)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "log_dir = os.path.join(f'Logs/BatchSizesTuning/Logs_train_test_split_batch_100')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)\n",
    "es = EarlyStopping(monitor='val_loss', patience=50)\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(LSTM(64, return_sequences=True, activation='tanh', input_shape=(1099, 1434)))\n",
    "# model.add(LSTM(128, return_sequences=True, activation='tanh')) \n",
    "# model.add(LSTM(32, return_sequences=False, activation='tanh')) \n",
    "# model.add(Dense(32, activation='softmax', kernel_regularizer='l2')) \n",
    "# model.add(Dense(drowsines_levels.shape[0], activation='softmax'))\n",
    "\n",
    "# model.summary()\n",
    "# model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train, epochs=500, batch_size=100, shuffle=True, callbacks=[tb_callback, es], validation_split = 0.2) \n",
    "\n",
    "model.save(f'Models/drowsines_levels_train_test_split_${date_string}.h5')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f9b46a6e",
   "metadata": {},
   "source": [
    "### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c00b3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "now = datetime.now()\n",
    "date_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "\n",
    "X = np.array(files)\n",
    "y = to_categorical(labels).astype(int)\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', patience=50)\n",
    "\n",
    "log_dir = os.path.join(f'Logs/Logs_{date_string}_CrossValid')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "cv_scores = []\n",
    "for train, test in kfold.split(X):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(64, return_sequences=True, activation='tanh', input_shape=(1100, 1434)))\n",
    "    model.add(LSTM(128, return_sequences=True, activation='tanh')) \n",
    "    model.add(LSTM(32, return_sequences=False, activation='tanh')) \n",
    "    model.add(Dense(64, activation='softmax', kernel_regularizer='l2')) \n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(drowsines_levels.shape[0], activation='softmax'))\n",
    "\n",
    "    model.summary()\n",
    "    model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    model.fit(X[train], y[train], epochs=1000, batch_size=250, shuffle=True, callbacks=[tb_callback, es], validation_split=0.2) \n",
    "\n",
    "    scores = model.evaluate(X[test], y[test], verbose=0)\n",
    "    print(f'Accuracy: {scores[1]*100}%')\n",
    "    cv_scores.append(scores[1])\n",
    "\n",
    "print(f'Cross-validation accuracy: {np.mean(cv_scores)*100:.2f}% +/- {np.std(cv_scores)*100:.2f}%')\n",
    "model.save(f'Models/drowsines_levels_cross_validation_${date_string}.h5')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9554c5cc",
   "metadata": {},
   "source": [
    "#### Find hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b8b365",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "now = datetime.now()\n",
    "date_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "\n",
    "X = np.array(files)\n",
    "y = to_categorical(labels).astype(int)\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', patience=50)\n",
    "\n",
    "log_dir = os.path.join(f'Logs/BatchSizesTuning/Logs_{date_string}_CrossValid')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "batch_sizes = [50, 100, 150, 200]\n",
    "batch_size_scores = []\n",
    "cv_scores = []\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "\n",
    "    for train, test in kfold.split(X):\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(64, return_sequences=True, activation='tanh', input_shape=(1000, 1434)))\n",
    "        model.add(LSTM(128, return_sequences=True, activation='tanh')) \n",
    "        model.add(LSTM(32, return_sequences=False, activation='tanh')) \n",
    "        model.add(Dense(32, activation='softmax', kernel_regularizer='l2')) \n",
    "        model.add(Dense(drowsines_levels.shape[0], activation='softmax'))\n",
    "\n",
    "        model.summary()\n",
    "        model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        model.fit(X[train], y[train], epochs=1000, batch_size=batch_size, shuffle=True, callbacks=[tb_callback, es], validation_split=0.2) \n",
    "\n",
    "        scores = model.evaluate(X[test], y[test], verbose=0)\n",
    "        print(f'Accuracy: {scores[1]*100}%')\n",
    "        cv_scores.append(scores[1])\n",
    "\n",
    "    cross_validation_accuracy = np.mean(cv_scores)*100\n",
    "    cross_validation_accuracy_error = np.std(cv_scores)*100\n",
    "    print(f'Cross-validation accuracy: {cross_validation_accuracy:.2f}% +/- {cross_validation_accuracy_error:.2f}% for batch size: {batch_size}')\n",
    "    batch_size_scores.append(dict(accuracy = cross_validation_accuracy, accuracy_error = cross_validation_accuracy_error, batch_size_used = batch_size))\n",
    "    \n",
    "    # model.save(f'Models/drowsines_levels_cross_validation_${date_string}.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57a0236",
   "metadata": {},
   "outputs": [],
   "source": [
    "for score in batch_size_scores:\n",
    "    print(score)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a7a45a3a",
   "metadata": {},
   "source": [
    "#### Grid search for parameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cc4d21eb",
   "metadata": {},
   "source": [
    "### Other architecture experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdf0ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "date_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "\n",
    "X = np.array(files)\n",
    "y = to_categorical(labels).astype(int)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "# es = EarlyStopping(monitor='val_loss', patience=50)\n",
    "\n",
    "log_dir = os.path.join(f'Logs_15')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True, activation='tanh', input_shape=(1000, 1434)))\n",
    "model.add(LSTM(128, return_sequences=True, activation='tanh')) \n",
    "model.add(TimeDistributed(Dropout(0.1)))\n",
    "model.add(LSTM(32, return_sequences=False, activation='tanh')) \n",
    "model.add(Dense(32, activation='softmax', kernel_regularizer=regularizers.l2(0.008))) \n",
    "model.add(Dense(drowsines_levels.shape[0], activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=500, batch_size=50, shuffle = True, callbacks=[tb_callback], validation_split = 0.2) \n",
    "\n",
    "model.save(f'drowsines_weights_noStop_l2008_drop01_${date_string}.h5')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ee74d36",
   "metadata": {},
   "source": [
    "## Model validation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "43a0ba45",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8904583",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m yhat \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(X_test)\n\u001b[1;32m      2\u001b[0m ytrue \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(y_test, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mtolist()\n\u001b[1;32m      3\u001b[0m yhat \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(yhat, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mtolist()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "def generateConfusionMatrix():\n",
    "    yhat = model.predict(X_test)\n",
    "    ytrue = np.argmax(y_test, axis=1).tolist()\n",
    "    yhat = np.argmax(yhat, axis=1).tolist()\n",
    "    multilabel_confusion_matrix(ytrue, yhat)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5aad2df5",
   "metadata": {},
   "source": [
    "### Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46b02c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir Logs/Logs_15/04/2023 21:41:01"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8479c1ae",
   "metadata": {},
   "source": [
    "## Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae682ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://machinelearningmastery.com/use-dropout-lstm-networks-time-series-forecasting/ - LSTM Dropout\n",
    "https://openai.com/research/how-ai-training-scales - OpenAI odnosnie bathc size\n",
    "https://towardsdatascience.com/simple-guide-to-hyperparameter-tuning-in-neural-networks-3fe03dad8594 - hyperparameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
