{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "492a3578",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ccd3d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Platform: macOS-13.1-arm64-arm-64bit\n",
      "Tensor Flow Version: 2.9.0\n",
      "Keras Version: 2.9.0\n",
      "\n",
      "Python 3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:25:29) [Clang 14.0.6 ]\n",
      "Scikit-Learn 1.2.0\n",
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "import sys\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Flatten, MaxPooling2D, TimeDistributed\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score\n",
    "import sklearn as sk\n",
    "from keras import regularizers\n",
    "import platform\n",
    "\n",
    "\n",
    "print(f\"Python Platform: {platform.platform()}\")\n",
    "print(f\"Tensor Flow Version: {tf.__version__}\")\n",
    "print(f\"Keras Version: {tensorflow.keras.__version__}\")\n",
    "print()\n",
    "print(f\"Python {sys.version}\")\n",
    "print(f\"Scikit-Learn {sk.__version__}\")\n",
    "gpu = len(tf.config.list_physical_devices('GPU'))>0\n",
    "print(\"GPU is\", \"available\" if gpu else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62eac5e",
   "metadata": {},
   "source": [
    "##  Load frames"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d799706f",
   "metadata": {},
   "source": [
    "### Old dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "baeb7dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File number: 1 processed\n",
      "File number: 2 processed\n",
      "File number: 3 processed\n",
      "File number: 4 processed\n",
      "File number: 5 processed\n",
      "File number: 6 processed\n",
      "File number: 7 processed\n",
      "File number: 8 processed\n",
      "File number: 9 processed\n",
      "File number: 10 processed\n"
     ]
    }
   ],
   "source": [
    "drowsines_levels = np.array([\"0\", \"5\", \"10\"])\n",
    "DATA_PATH = os.path.join('./Dataset')\n",
    "label_map = {label: num for num, label in enumerate(drowsines_levels)}\n",
    "\n",
    "files, labels = [], []\n",
    "\n",
    "for video_index in range(1,11):\n",
    "  for level in drowsines_levels:\n",
    "    window = []      \n",
    "    for frame in range(1, 4001, 2):\n",
    "      file_path = os.path.join(DATA_PATH, str(video_index), level, (str(frame) + \".npy\"))\n",
    "      if os.path.exists(file_path):\n",
    "        res = np.load(file_path)        \n",
    "        window.append(res)\n",
    "      else:\n",
    "        print(f\"File at {file_path} doesn't exists\")\n",
    "    files.append(window)\n",
    "    labels.append(label_map[level])\n",
    "  print(f\"File number: {video_index} processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5f439b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 2000, 1434)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check loaded files shape\n",
    "np.array(files).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d9b1b27b",
   "metadata": {},
   "source": [
    "### New dataset\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ecb697a6",
   "metadata": {},
   "source": [
    "#### Load new dataset pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f1c967",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "def load_data(DATA_PATH, drowsines_levels):\n",
    "    label_map = {label: num for num, label in enumerate(drowsines_levels)}\n",
    "    files, labels = [], []\n",
    "    pool = Pool(processes=4)\n",
    "    for level in drowsines_levels:\n",
    "        for video_index in range(1, 90):\n",
    "            file_paths = [os.path.join(DATA_PATH, level, str(video_index), (str(frame) + \".npy\")) for frame in range(1, 1001)]\n",
    "            results = pool.map(process_file, file_paths)\n",
    "            window = [result for result in results if result is not None]\n",
    "            files.append(window)\n",
    "            labels.append(label_map[level])\n",
    "            print(f\"File number: {video_index} processed for level {level}\")\n",
    "    return files, labels\n",
    "\n",
    "def process_file(file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        res = np.load(file_path)\n",
    "        return res\n",
    "    else:\n",
    "        print(f\"File at {file_path} doesn't exist\")\n",
    "\n",
    "drowsines_levels = np.array([\"Sleepy\", \"NonSleepy\"])\n",
    "DATA_PATH = os.path.join('./Extracted_Values_Bigger_Confidence')\n",
    "files, labels = load_data(DATA_PATH, drowsines_levels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e5ee6305",
   "metadata": {},
   "source": [
    "#### Load new dataset standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "312c1237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File number: 1 processed for level Sleepy\n",
      "File number: 2 processed for level Sleepy\n",
      "File number: 3 processed for level Sleepy\n",
      "File number: 4 processed for level Sleepy\n",
      "File number: 5 processed for level Sleepy\n",
      "File number: 6 processed for level Sleepy\n",
      "File number: 7 processed for level Sleepy\n",
      "File number: 8 processed for level Sleepy\n",
      "File number: 9 processed for level Sleepy\n",
      "File number: 10 processed for level Sleepy\n",
      "File number: 11 processed for level Sleepy\n",
      "File number: 12 processed for level Sleepy\n",
      "File number: 13 processed for level Sleepy\n",
      "File number: 14 processed for level Sleepy\n",
      "File number: 15 processed for level Sleepy\n",
      "File number: 16 processed for level Sleepy\n",
      "File number: 17 processed for level Sleepy\n",
      "File number: 18 processed for level Sleepy\n",
      "File number: 19 processed for level Sleepy\n",
      "File number: 20 processed for level Sleepy\n",
      "File number: 21 processed for level Sleepy\n",
      "File number: 22 processed for level Sleepy\n",
      "File number: 23 processed for level Sleepy\n",
      "File number: 24 processed for level Sleepy\n",
      "File number: 25 processed for level Sleepy\n",
      "File number: 26 processed for level Sleepy\n",
      "File number: 27 processed for level Sleepy\n",
      "File number: 28 processed for level Sleepy\n",
      "File number: 29 processed for level Sleepy\n",
      "File number: 30 processed for level Sleepy\n",
      "File number: 31 processed for level Sleepy\n",
      "File number: 32 processed for level Sleepy\n",
      "File number: 33 processed for level Sleepy\n",
      "File number: 34 processed for level Sleepy\n",
      "File number: 35 processed for level Sleepy\n",
      "File number: 36 processed for level Sleepy\n",
      "File number: 37 processed for level Sleepy\n",
      "File number: 38 processed for level Sleepy\n",
      "File number: 39 processed for level Sleepy\n",
      "File number: 40 processed for level Sleepy\n",
      "File number: 41 processed for level Sleepy\n",
      "File number: 42 processed for level Sleepy\n",
      "File number: 43 processed for level Sleepy\n",
      "File number: 44 processed for level Sleepy\n",
      "File number: 45 processed for level Sleepy\n",
      "File number: 46 processed for level Sleepy\n",
      "File number: 47 processed for level Sleepy\n",
      "File number: 48 processed for level Sleepy\n",
      "File number: 49 processed for level Sleepy\n",
      "File number: 50 processed for level Sleepy\n",
      "File number: 51 processed for level Sleepy\n",
      "File number: 52 processed for level Sleepy\n",
      "File number: 53 processed for level Sleepy\n",
      "File number: 54 processed for level Sleepy\n",
      "File number: 55 processed for level Sleepy\n",
      "File number: 56 processed for level Sleepy\n",
      "File number: 57 processed for level Sleepy\n"
     ]
    }
   ],
   "source": [
    "drowsines_levels = np.array([\"Sleepy\", \"NonSleepy\"])\n",
    "DATA_PATH = os.path.join('./Extracted_Values_Bigger_Confidence')\n",
    "label_map = {label: num for num, label in enumerate(drowsines_levels)}\n",
    "\n",
    "files, labels = [], []\n",
    "\n",
    "for level in drowsines_levels:\n",
    "    for video_index in range(1,90):\n",
    "        window = []      \n",
    "        for frame in range(1, 1001):\n",
    "            file_path = os.path.join(DATA_PATH, level, str(video_index), (str(frame) + \".npy\"))\n",
    "            if os.path.exists(file_path):\n",
    "                res = np.load(file_path)        \n",
    "                window.append(res)\n",
    "            else:\n",
    "                print(f\"File at {file_path} doesn't exists\")\n",
    "        files.append(window)\n",
    "        labels.append(label_map[level])\n",
    "        print(f\"File number: {video_index} processed for level {level}\")\n",
    "\n",
    "print(f\"Processed! Current dataset shape: {np.array(files).shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8fcc6821",
   "metadata": {},
   "source": [
    "#### Load new dataset fancy progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38c22c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 100.00% | Level: NonSleepy\r"
     ]
    }
   ],
   "source": [
    "def processDataset():\n",
    "    drowsines_levels = np.array([\"Sleepy\", \"NonSleepy\"])\n",
    "    DATA_PATH = os.path.join('./Extracted_Values_Bigger_Confidence')\n",
    "    label_map = {label: num for num, label in enumerate(drowsines_levels)}\n",
    "\n",
    "    files, labels = [], []\n",
    "\n",
    "    for level in drowsines_levels:\n",
    "        for video_index in range(1,90):\n",
    "            window = []      \n",
    "            for frame in range(1, 1001):\n",
    "                file_path = os.path.join(DATA_PATH, level, str(video_index), (str(frame) + \".npy\"))\n",
    "                if os.path.exists(file_path):\n",
    "                    res = np.load(file_path)        \n",
    "                    window.append(res)\n",
    "                else:\n",
    "                    print(f\"File at {file_path} doesn't exists\")\n",
    "            files.append(window)\n",
    "            labels.append(label_map[level])\n",
    "            progress_percentage = (video_index / 89) * 100  # Calculate the percentage progress\n",
    "            status = f\"Progress: {progress_percentage:.2f}% | Level: {level}\"\n",
    "            print(status, end=\"\\r\")  # Overwrite the previous status\n",
    "\n",
    "    print(f\"Processed! Current dataset shape: {np.array(files).shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcfbd964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178, 1000, 1434)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(files).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e388f0e6",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9ca64322",
   "metadata": {},
   "source": [
    "#### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9be1ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True, activation='tanh', input_shape=(1049, 1434)))\n",
    "model.add(LSTM(128, return_sequences=True, activation='tanh')) \n",
    "model.add(LSTM(32, return_sequences=False, activation='tanh')) \n",
    "model.add(Dense(64, activation='softmax', kernel_regularizer='l2')) \n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(drowsines_levels.shape[0], activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "22460caf",
   "metadata": {},
   "source": [
    "#### Train Test Split Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "date_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "\n",
    "X = np.array(files)\n",
    "y = to_categorical(labels).astype(int)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "log_dir = os.path.join(f'Logs_{date_string}_train_test_split')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)\n",
    "es = EarlyStopping(monitor='val_loss', patience=50)\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(LSTM(64, return_sequences=True, activation='tanh', input_shape=(1099, 1434)))\n",
    "# model.add(LSTM(128, return_sequences=True, activation='tanh')) \n",
    "# model.add(LSTM(32, return_sequences=False, activation='tanh')) \n",
    "# model.add(Dense(32, activation='softmax', kernel_regularizer='l2')) \n",
    "# model.add(Dense(drowsines_levels.shape[0], activation='softmax'))\n",
    "\n",
    "# model.summary()\n",
    "# model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train, epochs=500, batch_size=100, shuffle=True, callbacks=[tb_callback, es], validation_split = 0.2) \n",
    "\n",
    "model.save(f'Models/drowsines_levels_train_test_split_${date_string}.h5')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f9b46a6e",
   "metadata": {},
   "source": [
    "### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c00b3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "now = datetime.now()\n",
    "date_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "\n",
    "X = np.array(files)\n",
    "y = to_categorical(labels).astype(int)\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', patience=50)\n",
    "\n",
    "log_dir = os.path.join(f'Logs/Logs_{date_string}_CrossValid')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "cv_scores = []\n",
    "for train, test in kfold.split(X):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(64, return_sequences=True, activation='tanh', input_shape=(1100, 1434)))\n",
    "    model.add(LSTM(128, return_sequences=True, activation='tanh')) \n",
    "    model.add(LSTM(32, return_sequences=False, activation='tanh')) \n",
    "    model.add(Dense(64, activation='softmax', kernel_regularizer='l2')) \n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(drowsines_levels.shape[0], activation='softmax'))\n",
    "\n",
    "    model.summary()\n",
    "    model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    model.fit(X[train], y[train], epochs=1000, batch_size=250, shuffle=True, callbacks=[tb_callback, es], validation_split=0.2) \n",
    "\n",
    "    scores = model.evaluate(X[test], y[test], verbose=0)\n",
    "    print(f'Accuracy: {scores[1]*100}%')\n",
    "    cv_scores.append(scores[1])\n",
    "\n",
    "print(f'Cross-validation accuracy: {np.mean(cv_scores)*100:.2f}% +/- {np.std(cv_scores)*100:.2f}%')\n",
    "model.save(f'Models/drowsines_levels_cross_validation_${date_string}.h5')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9554c5cc",
   "metadata": {},
   "source": [
    "# Find hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b8b365",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "now = datetime.now()\n",
    "date_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "\n",
    "X = np.array(files)\n",
    "y = to_categorical(labels).astype(int)\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', patience=50)\n",
    "\n",
    "log_dir = os.path.join(f'Logs/Logs_{date_string}_CrossValid')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "batch_sizes = [50, 100, 150, 200, 250]\n",
    "\n",
    "cv_scores = []\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "\n",
    "    for train, test in kfold.split(X):\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(64, return_sequences=True, activation='tanh', input_shape=(1099, 1434)))\n",
    "        model.add(LSTM(128, return_sequences=True, activation='tanh')) \n",
    "        model.add(LSTM(32, return_sequences=False, activation='tanh')) \n",
    "        model.add(Dense(64, activation='softmax', kernel_regularizer='l2')) \n",
    "        model.add(Dropout(0.1))\n",
    "        model.add(Dense(drowsines_levels.shape[0], activation='softmax'))\n",
    "\n",
    "        model.summary()\n",
    "        model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        model.fit(X[train], y[train], epochs=1000, batch_size=199, shuffle=True, callbacks=[tb_callback, es], validation_split=0.2) \n",
    "\n",
    "        scores = model.evaluate(X[test], y[test], verbose=0)\n",
    "        print(f'Accuracy: {scores[1]*100}%')\n",
    "        cv_scores.append(scores[1])\n",
    "\n",
    "    cross_validation_accuracy = np.mean(cv_scores)*100\n",
    "    cross_validation_accuracy_error = np.std(cv_scores)*100\n",
    "    print(f'Cross-validation accuracy: {cross_validation_accuracy:.2f}% +/- {cross_validation_accuracy_error:.2f}% for batch size: {batch_size}')\n",
    "    \n",
    "    # model.save(f'Models/drowsines_levels_cross_validation_${date_string}.h5')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cc4d21eb",
   "metadata": {},
   "source": [
    "### Other architecture experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdf0ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "date_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "\n",
    "X = np.array(files)\n",
    "y = to_categorical(labels).astype(int)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "# es = EarlyStopping(monitor='val_loss', patience=50)\n",
    "\n",
    "log_dir = os.path.join(f'Logs_15')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True, activation='tanh', input_shape=(1000, 1434)))\n",
    "model.add(LSTM(128, return_sequences=True, activation='tanh')) \n",
    "model.add(TimeDistributed(Dropout(0.1)))\n",
    "model.add(LSTM(32, return_sequences=False, activation='tanh')) \n",
    "model.add(Dense(32, activation='softmax', kernel_regularizer=regularizers.l2(0.008))) \n",
    "model.add(Dense(drowsines_levels.shape[0], activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=500, batch_size=50, shuffle = True, callbacks=[tb_callback], validation_split = 0.2) \n",
    "\n",
    "model.save(f'drowsines_weights_noStop_l2008_drop01_${date_string}.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee74d36",
   "metadata": {},
   "source": [
    "## Model validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a0ba45",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8904583",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m yhat \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(X_test)\n\u001b[1;32m      2\u001b[0m ytrue \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(y_test, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mtolist()\n\u001b[1;32m      3\u001b[0m yhat \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(yhat, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mtolist()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "def generateConfusionMatrix():\n",
    "    yhat = model.predict(X_test)\n",
    "    ytrue = np.argmax(y_test, axis=1).tolist()\n",
    "    yhat = np.argmax(yhat, axis=1).tolist()\n",
    "    multilabel_confusion_matrix(ytrue, yhat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aad2df5",
   "metadata": {},
   "source": [
    "### Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46b02c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir Logs/Logs_15/04/2023 21:41:01"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8479c1ae",
   "metadata": {},
   "source": [
    "## Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae682ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://machinelearningmastery.com/use-dropout-lstm-networks-time-series-forecasting/ - LSTM Dropout\n",
    "https://openai.com/research/how-ai-training-scales - OpenAI odnosnie bathc size"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
